{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Question-1-[30-marks]\" data-toc-modified-id=\"Question-1-[30-marks]-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Question 1 [30 marks]</a></span></li><li><span><a href=\"#Question-2-[30-marks]\" data-toc-modified-id=\"Question-2-[30-marks]-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Question 2 [30 marks]</a></span></li><li><span><a href=\"#Question-3-[40-marks]\" data-toc-modified-id=\"Question-3-[40-marks]-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Question 3 [40 marks]</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Mathematics <a class=\"tocSkip\">\n",
    "\n",
    "## 2023/24 Assessment - solutions <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release: 10AM Thursday 2 November 2023 <a class=\"tocSkip\">\n",
    "\n",
    "## Deadline: 5PM Friday 3 November 2023 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions  <a class=\"tocSkip\">\n",
    "    \n",
    "\n",
    "- This is the assessed coursework for module \"Computational Mathematics\" for year 2023/24.\n",
    "\n",
    "\n",
    "- **This is independent work, you cannot work in groups.**\n",
    "\n",
    "\n",
    "- This notebook contains three questions. Please answer all questions.\n",
    "\n",
    "\n",
    "- You will be assessed on both your code as well as your supporting descriptions/discussions of the results you obtain.\n",
    "\n",
    "\n",
    "- You may use any library we used in lectures, and you may reuse any code from lectures and homeworks. If you use an algorithm not explicitly mentioned in the question, then please explain which algorithm you are using and why you have chosen it.\n",
    "\n",
    "\n",
    "- You should submit your solutions as a single self-contained Jupyter notebooks via Github classroom - **please submit a version where you have run all of the cells and all the outputs/plots etc are visible without the marker having to run the notebook themselves**.\n",
    "    \n",
    "    \n",
    "\n",
    "- If you are happy to write down your mathematical work in the form of Latex directly within this notebook then feel free to do this.\n",
    "\n",
    "\n",
    "- Alternatively you may if you choose submit parts of your answers via **scans or photos of hand written pages** - feel free to use this option in particular for supplementary sketches, equations or where I have asked you to complete a task with \"pen and paper\". You will not be marked down for not embedding equations into your markdown cells. Please make sure that the question number each sheet you scan refers to is very clear. Even if you embed the images within the notebook, please also commit the images files to your github submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# update as necessary for your solutions\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sl\n",
    "import scipy.interpolate as si\n",
    "from mpltools import annotation\n",
    "import scipy.integrate\n",
    "from scipy.interpolate import interp1d, CubicSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 [30 marks]\n",
    "\n",
    "Consider the following two statements\n",
    "\n",
    "<br>\n",
    "\n",
    "1. All models are wrong, but some are useful.\n",
    "\n",
    "\n",
    "2. A model may get the right answer for the wrong reason.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Where here we interpret a \"model\" as a piece of code written to provide a synthetic representation of some real world phenomena.\n",
    "\n",
    "<br>\n",
    "\n",
    "- (a) Explain what each of these two statements means and why they are important. \n",
    "\n",
    "<br>\n",
    "\n",
    "- (b) What can we as \"model\" developers do to mitigate the issues that may be implied by these two statements.\n",
    "\n",
    "<br>\n",
    "\n",
    "- (c) Explain the concepts of code verification, solution verification, model validation and model calibration. Use your own invented example to support your answers, where your model can be written in the form of a solution to a linear system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A)\n",
    "\n",
    "These two statements are important to have a good foundation in the fundamentals of data science and modelling.\n",
    "\n",
    "The first statement \"All models are wrong, but some are useful\" reflects the idea that no matter how \"complicated\" or sophisticated a model is, it will never exactly replicate real life. This is because a model by nature makes simplifications and approximations in order to help achieve any kind of a solution. This is why the first part of the phrase is important as it emphasizes the fact that no matter how much detail put into creating a model it can never be truly accurate and it should never be considered as such.\n",
    "\n",
    "However, the second part of the statement recognises that despite the flaws of models in replicating the real world exactly, they are useful and valuable if it can capture enough key features of a system to extract approximate data or allow us to observe patterns of behaviour. How useful a model is will depend on the accuracy required in the predictions made or how well of an understanding it provides in how different variables will behave.\n",
    "\n",
    "This statement is important because it cautions against being overly confident in a model as it will never reflect the true complexities of life but at the same time appreciate how valuable the extracted data can be when working with models.\n",
    "\n",
    "The second statement \"A model may get the right answer for the wrong reason\" reflects the fact that a model can give accurate/correct predictions but the reason they do so might not be routed in correct logic.A model might for example give accurate outcomes for a certain set of variables based on a pattern that does not actually exist or represent the principles of the system being modelled. If this model is then used to provide an outcome for variables outside of the scope of the intial pattern it will usually fail. This is called overfitting which is just one of the possible ways a model might make correct predictions for a certain set of consitions but not others.\n",
    "\n",
    "This statement is important as it emphasises the need to understand and validate why a model works rather than accept that it works based on a limited range of conditions. This principle is fundamental in ensuring a model is reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B)\n",
    "In order to mitigate the risks and issues implied from the two statementsa several steps can be taken. Before beginning to understand a model it is important that the domain is understood well. This involves understanding the data, process and the assumptions used to simplify and represent the system. For example if modelling fluid flow it is helpful to understand boundary conditions, the type of flow and various other properties and their implications on the model. With a thorough understanding of the domain, such as fluid dynamics, the right equations can be chosen (Navier-Stokes, Euler, potential flow, etc.) and numerical methods (finite difference, finite element, boundary element, etc.) that best represent the system that is being modelled.\n",
    "\n",
    "While not always feasible, structuring a model on well established theories and using emprical evidence that outlines the way the system works well. Using a theoretical framework helps ensure that the model closely reflects the mechanisms and interactions in real life. Empirical evidence ensures that a model is validated using real world data making sure it is practically applicable.\n",
    "\n",
    "Another issue that can be caused from a lack of true consideration of the statements is overfitting. Using only one specific data set makes the model susceptible to working due the noise and not the underlying logic. A diverse dataset helps the model to distinguish between the underlying signal (true pattern) and noise (random or irrelevant patterns). Therefore it is very important that a variety of data sources are utiised in order to reduce the possibility of overfitting. Overfitting can lead to models that work well with some specific data but outside of this begins to fail. So techniques such as regularisation or simplifiying the model can also help reduce this issue.\n",
    "\n",
    "Once all of these are considered it is very important to rigorously validate and test the models output against analytical and unseen data. A sensitivity analysis is also vital to help idenitfy which variables or assumptions affect the output of the model and how significant this impact is. This validationa and analysis ensures that the model is robust.\n",
    "\n",
    "In the construction of mathematical models, ensuring that they are understandable is just as critical as their precision and stability. A model that can be easily interpreted engenders trust by making its inner workings clear, which also simplifies the process of refining and troubleshooting. Furthermore, it's important for the model to undergo regular reviews and updates, which keeps it relevant and accurate as new data comes to light and circumstances evolve. Ethical considerations cannot be overlooked either; it's imperative to examine models for biases that could negatively affect outcomes and therefore the users of the model. Integrating ethical considerations into the modeling process is a safeguard against the inadvertent reinforcement of existing biases, preventing unjust or detrimental outcomes. These three aspectsâ€”clarity, adaptability, and ethical responsibility are essential practices for the application of mathematical models (not just mathematical) to real-world problems.\n",
    "\n",
    "Whilst all of the above can reduce the issues it is important to once again acknowledge the statement and undertsand that while a models accuracy can be improved it can never be truly accurate.Therefore any \"model\" developer will acknowledge and quantify the uncertainty in the model. This can be done through any number of methods such as confidence intervals or Bayesian statistical methods and many more. This reduced overconfidence in a model and allows the model to be considered for the approximation it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C)\n",
    "**Code Verification**: Code verification is the process of ensuring that the code written (in this case to implement a model of a system) accuractely implements the intended algorithms and mathematical models. It is important to note the process only checks whether the code reflects the model correctly and does not check if the model represents the real world system well.\n",
    "\n",
    "**Solution Verification**: Solution verification is the process of ensuring that the output from the code is accurate to a sufficient degree. This involves chekcing the numerical solutions obtained against analytical solutions but can also involve a convergence study so that the solution does not change significantly with different mesh-sizes/discretisations.\n",
    "\n",
    "**Model Validation**: Model validation is used to confirm that the model represents the system accurately enough for its purpose. Once again this is done by comparing outputs from the model with experimental/analytical data or observations that have been made. This is different to solution verification because the code is not necessarily being checked here but rather the eqautions that model the system.\n",
    "\n",
    "**Model Calibration**: Model calibration involves adjusting the model and its paramters so that the output from the model aligns with observed data as much as possible. In short it is an iterative process of fine tuning a model to be more accurate based on observations from previous iterations until the accuracy is sufficient.\n",
    "\n",
    "\n",
    "An example of the above could be shown by the modelling a system for a mass attached to a spring with a sprink constant $K$. This is system is subject to an external force $F$ and the model is trying to find the displacement $x$.\n",
    "\n",
    "The static equilibrium equation (the system is stationary) can be written as:<br>\n",
    "$$F = kx$$\n",
    "<br> The mass will therefore displace by:<br>\n",
    "$$x = \\frac{F}{k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the spring constant and the external force\n",
    "k = 200  # N/m\n",
    "F = 1000  # N\n",
    "\n",
    "# Step 1: Code Verification\n",
    "# Solve for x using the equation x = F / k\n",
    "x_computed = F / k\n",
    "\n",
    "# Step 2: Solution Verification\n",
    "# Verify the solution by checking if it satisfies the equation kx = F\n",
    "verification_result = np.isclose(k * x_computed, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the measured data for this system that suggests the displacement should be 4.8 m for a 1000 N force. A K value that represents such a system is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed displacement: 5.0 m\n",
      "Verification: Passed\n",
      "Validation: Failed\n",
      "Calibrated k: 208.33333333333334 N/m\n",
      "Calibrated displacement: 4.8 m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Model Validation\n",
    "x_measured = 4.8\n",
    "validation_result = np.isclose(x_computed, x_measured)\n",
    "\n",
    "# Step 4: Model Calibration\n",
    "# Adjust the spring constant so that the spring constant gives the \"measured\" outpt\n",
    "k_calibrated = F / x_measured\n",
    "\n",
    "# Recalculate the displacement\n",
    "x_calibrated = F / k_calibrated\n",
    "\n",
    "\n",
    "print(f\"Computed displacement: {x_computed} m\")\n",
    "print(\"Verification: \" + (\"Passed\" if verification_result else \"Failed\"))\n",
    "print(\"Validation: \" + (\"Passed\" if validation_result else \"Failed\"))\n",
    "print(f\"Calibrated k: {k_calibrated} N/m\")\n",
    "print(f\"Calibrated displacement: {x_calibrated} m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 [30 marks]\n",
    "\n",
    "Consider a function of one independent variable: $f(x)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "The first-order forward difference approximation to the derivative of $f$ at location $x$ is given by\n",
    "\n",
    "$$ f'(x)\\approx \\frac{f(x+\\Delta x)-f(x)}{\\Delta x} $$\n",
    "\n",
    "The second-order central difference approximation to the derivative of $f$ at location $x$ is given by \n",
    "\n",
    "$$ f'(x)\\approx \\frac{f(x+\\Delta x)-f(x - \\Delta x)}{2 \\Delta x} $$\n",
    "\n",
    "The fourth-order central difference approximation to the derivative of $f$ at location $x$ is given by\n",
    "\n",
    "$$ f'(x)\\approx \\frac{-f(x + 2\\Delta x) + 8 f(x+\\Delta x)- 8 f(x - \\Delta x) + f(x - 2\\Delta x)}{12 \\Delta x} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "- Use Taylor series analyses to confirm the orders of accuracies of the above approximations.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Implement these approximations in code, and verify your implementations using the fact that\n",
    "\n",
    "$$\\left. \\frac{d}{dx} \\exp (x) \\right|_{x=1} = \\exp(1) $$\n",
    "\n",
    "- Explain why such a verification exercise give you faith that your implementations are correct.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Demonstrate what happens if you take $\\Delta x$ values that are too large or too small, i.e. as $\\Delta x\\rightarrow 0 $ and $\\Delta x\\rightarrow \\infty $. Comment on whether the behaviour you observe is what you expect, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor series analyses\n",
    "\n",
    "#### 1. First-order forward difference NEEDS TO BE REDONE:\n",
    "Taylor series expansion for $f(x + \\Delta x)$ (forward difference):\n",
    "$$f(x + \\Delta x) = f(x) + f'(x)\\Delta x + \\frac{f''(x)(\\Delta x)^2}{2!} + \\mathcal{O}(\\Delta x^3)$$\n",
    "\n",
    "Substitute this expansion into the forward difference formula and truncating third order and higher terms:\n",
    "$$ f'(x)\\approx \\frac{f(x) + f'(x)\\Delta x + \\frac{f''(x)(\\Delta x)^2}{2!}-f(x)}{\\Delta x} $$\n",
    "<br>\n",
    "$$ f'(x)\\approx \\frac{\\require{cancel}\\cancel{f(x)} + f'(x)\\cancel{\\Delta x} + \\frac{f''(x)(\\Delta x)\\cancel{^2}}{2!}-\\cancel{f(x)}}{\\cancel{\\Delta x}} $$\n",
    "\n",
    "$$f'(x) \\approx f'(x) + \\frac{f''(x)\\Delta x}{2}$$\n",
    "\n",
    "The leading error can be seen to be $\\frac{f''(x)\\Delta x}{2}$ which confirms that it is of first order since $\\Delta x$ is raised to the power of 1\n",
    "\n",
    "#### 2. Second-order central difference:\n",
    "Expansion for forward difference and backward difference:\n",
    "\n",
    "\\begin{align*}\n",
    "f(x_0+ \\Delta x) &= f(x_0)+\\Delta x f'(x_0)+\\frac{\\Delta x^2}{2}f''(x_0) + \\mathcal{O}(\\Delta x^3)\\\\[5pt]\n",
    "f(x_0- \\Delta x) &= f(x_0)- \\Delta x f'(x_0)+\\frac{(-\\Delta x)^2}{2}f''(x_0) + \\mathcal{O}((-\\Delta x)^3).\n",
    "\\end{align*}\n",
    "\n",
    "$(-\\Delta x)^2=\\Delta x^2$ , and for $\\mathcal{O}$ the signs wont be considered:\n",
    "\n",
    "\\begin{align*} \n",
    "f(x_0+\\Delta x) &= f(x_0)+\\Delta xf'(x_0)+\\frac{\\Delta x^2}{2}f''(x_0) + \\mathcal{O}(\\Delta x^3),\\\\[5pt]\n",
    "f(x_0-\\Delta x) &= f(x_0)-\\Delta xf'(x_0)+\\frac{\\Delta x^2}{2}f''(x_0) + \\mathcal{O}(\\Delta x^3).\n",
    "\\end{align*}\n",
    "\n",
    "In order to rearrange for $f'(x_0)$ the two equations are subtracted from each other:\n",
    "\n",
    "$$ f(x_0+\\Delta x)-f(x_0-\\Delta x)= [f(x_0)+\\Delta xf'(x_0)+\\frac{\\Delta x^2}{2}f''(x_0) + \\mathcal{O}(\\Delta x^3)] - [f(x_0)-\\Delta xf'(x_0)+\\frac{\\Delta x^2}{2}f''(x_0) + \\mathcal{O}(\\Delta x^3)]$$\n",
    "<br>\n",
    "$$f(x_0+\\Delta x)-f(x_0-\\Delta x) = \\left[\\require{cancel}\\cancel{f(x_0)}+\\Delta xf'(x_0)+\\cancel{\\frac{\\Delta x^2}{2}f''(x_0)} + {\\mathcal{O}(\\Delta x^3)}\\right] - \\left[\\require{cancel}\\cancel{f(x_0)}-\\Delta xf'(x_0)+\\cancel{\\frac{\\Delta x^2}{2}f''(x_0)} + {\\mathcal{O}(\\Delta x^3)}\\right]$$\n",
    "\n",
    "<br>\n",
    "$$ f(x_0+\\Delta x)-f(x_0-\\Delta x)=2\\Delta xf'(x_0) + \\mathcal{O}(\\Delta x^3)$$\n",
    "\n",
    "This is rearranged to get an expression for $f'(x_0)$:\n",
    "\n",
    "$$ f'(x_0)=\\frac{f(x_0+\\Delta x)-f(x_0-\\Delta x)}{2\\Delta x} + \\mathcal{O}(\\Delta x^2),$$\n",
    "\n",
    "Since the leading error term is $\\mathcal{O}(\\Delta x^2)$ the central difference formula is of second order accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fourth-order central difference:\n",
    "Taylor series expansion for forward difference and backward difference for 1 and 2 steps $f(x_0\\pm\\Delta x)$ and $f(x_0\\pm\\Delta 2x)$:\n",
    "\n",
    "$$f(x + \\Delta x) \\approx f(x) + \\Delta x f'(x) + \\frac{\\Delta x^2}{2!} f''(x) + \\frac{\\Delta x^3}{3!} f'''(x) + \\frac{\\Delta x^4}{4!} f''''(x) + \\mathcal{O}((\\Delta x)^5)\n",
    "$$\n",
    "\n",
    "$$f(x - \\Delta x) \\approx f(x) - \\Delta x f'(x) + \\frac{\\Delta x^2}{2!} f''(x) - \\frac{\\Delta x^3}{3!} f'''(x) + \\frac{\\Delta x^4}{4!} f''''(x) + \\mathcal{O}((\\Delta x)^5)\n",
    "$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x_0 + 2\\Delta x) &= f(x_0) + 2\\Delta x f'(x_0) + \\frac{(2\\Delta x)^2}{2!}f''(x_0) + \\frac{(2\\Delta x)^3}{3!}f'''(x_0) + \\frac{(2\\Delta x)^4}{4!}f''''(x_0) + \\mathcal{O}((\\Delta x)^5).\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x_0 - 2\\Delta x) &= f(x_0) - 2\\Delta x f'(x_0) + \\frac{(-2\\Delta x)^2}{2!}f''(x_0) - \\frac{(-2\\Delta x)^3}{3!}f'''(x_0) + \\frac{(-2\\Delta x)^4}{4!}f''''(x_0) + \\mathcal{O}((\\Delta x)^5).\n",
    "\\end{align*}$$\n",
    "<br>\n",
    "These need to be substituted into the fourth order formula above but it would be easier to see cancel out some terms first.\n",
    "<br>Cancel out the terms for $8[f(x_0 + \\Delta x) - f(x_0 - \\Delta x)]:$\n",
    "\n",
    "$$\n",
    "8f(x + \\Delta x) - 8f(x - \\Delta x) \\approx \\\\\n",
    "\\cancel{8f(x)} + 8\\Delta x f'(x) + \\cancel{\\frac{8\\Delta x^2}{2!} f''(x)} + \\cancel{\\frac{8\\Delta x^3}{3!} f'''(x)} + \\cancel{\\frac{8\\Delta x^4}{4!} f''''(x)} \\\\\n",
    "- \\left(\\cancel{8f(x)} - 8\\Delta x f'(x) + \\cancel{\\frac{8\\Delta x^2}{2!} f''(x)} - \\cancel{\\frac{8\\Delta x^3}{3!} f'''(x)} + \\cancel{\\frac{8\\Delta x^4}{4!} f''''(x)}\\right) \\\\\n",
    "= 16\\Delta x f'(x)\n",
    "$$\n",
    "\n",
    "Cancel terms in $-f(x_0 + 2\\Delta x) + f(x_0 - 2\\Delta x)$:\n",
    "\n",
    "$$\n",
    "\\cancel{f(x_0)} - 2\\Delta x f'(x_0) + \\cancel{\\frac{(-2\\Delta x)^2}{2!}f''(x_0)} \\cancel{- \\frac{(-2\\Delta x)^3}{3!}f'''(x_0)} + \\cancel{\\frac{(-2\\Delta x)^4}{4!}f''''(x_0)} - \\\\  \\cancel{f(x_0)} + 2\\Delta x f'(x_0) + \\cancel{\\frac{(2\\Delta x)^2}{2!}f''(x_0)} + \\cancel{\\frac{(2\\Delta x)^3}{3!}f'''(x_0)} + \\cancel{\\frac{(2\\Delta x)^4}{4!}f''''(x_0)} \\\\\n",
    "= -4\\Delta x f'(x)\n",
    "$$\n",
    "\n",
    "combining the two sets of cancelled terms gives:\n",
    "$$\n",
    "12\\Delta x f'(x) + O(\\Delta x^5)\n",
    "$$\n",
    "\n",
    "now this can be substitued into the fourth-order central difference formula to give\n",
    "$$\n",
    "f'(x)\\approx \\frac{\\cancel{12\\Delta x }f'(x)}{\\cancel{12 \\Delta x}} + O(\\Delta x^4)\n",
    "$$\n",
    "The leading error term here is $O(\\Delta x^4)$ confirming the fourth-order accuracy of the central difference approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Value:  2.718281828459045\n",
      "First order forward: 2.7196414225332255\n",
      "Second order central: 2.718282281505724\n",
      "Fourth order central: 2.7182818284586054\n",
      "\n",
      "First order error: 1.3596e-03\n",
      "Second order error: 4.5305e-07\n",
      "Fourth order error: 4.3965e-13\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The first order error squared is not close to the second order error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFourth order error:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.4e}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fourth_order_error))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Assert that the magnitude of the errors are related\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(first_order_error\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, second_order_error, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first order error squared is not close to the second order error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(first_order_error\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m, fourth_order_error, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first order error**4 is not close to the fourth order error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(second_order_error\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, fourth_order_error, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe second order error**2 is not close to the fourth order error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The first order error squared is not close to the second order error."
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "def first_order_forward_diff(f, x, dx):\n",
    "    return (f(x + dx) - f(x)) / dx\n",
    "\n",
    "def second_order_central_diff(f, x, dx):\n",
    "    return (f(x + dx) - f(x - dx)) / (2 * dx)\n",
    "\n",
    "def fourth_order_central_diff(f, x, dx):\n",
    "    return (-f(x + 2 * dx) + 8 * f(x + dx) - 8 * f(x - dx) + f(x - 2 * dx)) / (12 * dx)\n",
    "\n",
    "# Choose a small delta x in order to approximate \n",
    "dx = 0.001\n",
    "\n",
    "# Calculate the approximations for the derivative of e^x at x=1\n",
    "x = 1\n",
    "first_order_approx = first_order_forward_diff(f, x, dx)\n",
    "second_order_approx = second_order_central_diff(f, x, dx)\n",
    "fourth_order_approx = fourth_order_central_diff(f, x, dx)\n",
    "\n",
    "exact_value = np.exp(1)\n",
    "\n",
    "print(\"Exact Value: \", exact_value)\n",
    "print(\"First order forward:\", first_order_approx)\n",
    "print(\"Second order central:\", second_order_approx)\n",
    "print(\"Fourth order central:\", fourth_order_approx)\n",
    "print(\"\")\n",
    "# Calculate the errors\n",
    "first_order_error = np.abs(first_order_approx - exact_value)\n",
    "second_order_error = np.abs(second_order_approx - exact_value)\n",
    "fourth_order_error = np.abs(fourth_order_approx - exact_value)\n",
    "\n",
    "print(\"First order error:\",\"{:.4e}\".format(first_order_error))\n",
    "print(\"Second order error:\",\"{:.4e}\".format(second_order_error))\n",
    "print(\"Fourth order error:\",\"{:.4e}\".format(fourth_order_error))\n",
    "\n",
    "# Assert that the magnitude of the errors are related\n",
    "\n",
    "assert np.isclose(first_order_error**2, second_order_error, rtol=1e-10), \"The first order error squared is not close to the second order error.\"\n",
    "\n",
    "assert np.isclose(first_order_error**4, fourth_order_error, rtol=1e-10), \"The first order error**4 is not close to the fourth order error.\"\n",
    "\n",
    "assert np.isclose(second_order_error**2, fourth_order_error, rtol=1e-10), \"The second order error**2 is not close to the fourth order error.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a smaller delta x in order to approximate more accurately \n",
    "dx = 0.00001\n",
    "\n",
    "# Calculate the approximations for the derivative of e^x at x=1\n",
    "x = 1\n",
    "first_order_approx = first_order_forward_diff(f, x, dx)\n",
    "second_order_approx = second_order_central_diff(f, x, dx)\n",
    "fourth_order_approx = fourth_order_central_diff(f, x, dx)\n",
    "\n",
    "exact_value = np.exp(1)\n",
    "\n",
    "print(\"Exact Value: \", exact_value)\n",
    "print(\"First order forward:\", first_order_approx)\n",
    "print(\"Second order central:\", second_order_approx)\n",
    "print(\"Fourth order central:\", fourth_order_approx)\n",
    "print(\"\")\n",
    "# Calculate the errors\n",
    "first_order_error = np.abs(first_order_approx - exact_value)\n",
    "second_order_error = np.abs(second_order_approx - exact_value)\n",
    "fourth_order_error = np.abs(fourth_order_approx - exact_value)\n",
    "\n",
    "print(\"First order error:\",\"{:.4e}\".format(first_order_error))\n",
    "print(\"Second order error:\",\"{:.4e}\".format(second_order_error))\n",
    "print(\"Fourth order error:\",\"{:.4e}\".format(fourth_order_error))\n",
    "\n",
    "# Assert that the magnitude of the errors are related\n",
    "\n",
    "assert np.isclose(first_order_error**2, second_order_error, rtol=1e-10), \"The first order error squared is not close to the second order error.\"\n",
    "\n",
    "assert np.isclose(first_order_error**4, fourth_order_error, rtol=1e-10), \"The first order error**4 is not close to the fourth order error.\"\n",
    "\n",
    "assert np.isclose(second_order_error**2, fourth_order_error, rtol=1e-10), \"The second order error**2 is not close to the fourth order error.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific verification exercise is significant in showing the validity of the derived approximations because the exponential function used has a well know derivate. This can be thought of as its theoretical solution and is excellent for testing. By comparing the results with the analytical solution at $x=1$ the accuracy of the model is also able to be established.\n",
    "\n",
    "The verification exercise also showed the derived approximations and consistent with theoretical observations.One obsevarion, for example,is that a fourth-order method is expected to be far more accurate than that of a first-order method and this can clearly be seen in the exercise which reinforces the correctness of the approximations. Another theoretical observation is the error scaling. Different order methods are meant to scale a certain way, for example since the error in the first-order solution should be proportional to $\\Delta x$ the error in the fourth-order solution ${\\Delta x}^4$. There were assertion checks in place to ensure this and as the $\\Delta x$ decreases this scaling can be confirmed to occur. Similarly, for a third theoretical observation, it is expected that as $\\Delta x$ decreases the error should reduce until a certain point. This indicates a correct implmentation of the approximation.\n",
    "\n",
    "While the verification exercise suggests that the implementation of derived appromixations is correct it does not completely gurantee that the derivations are correct as a single verification is not enough to show this. A range of functions with different behaviours whose analytical solutions are avialbe should be tested to ensure this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate all approximations and their errors\n",
    "def calculate_all_approximations(f, x, dx):\n",
    "    first_order = first_order_forward_diff(f, x, dx)\n",
    "    second_order = second_order_central_diff(f, x, dx)\n",
    "    fourth_order = fourth_order_central_diff(f, x, dx)\n",
    "    return (\n",
    "        np.abs(first_order - exact_value),\n",
    "        np.abs(second_order - exact_value),\n",
    "        np.abs(fourth_order - exact_value)\n",
    "    )\n",
    "\n",
    "# Calculate the errors for all methods across the range of delta x values\n",
    "first_order_errors = []\n",
    "second_order_errors = []\n",
    "fourth_order_errors = []\n",
    "\n",
    "dx_values = np.logspace(-15, 0, 100)  # delta x values from 10^-15 to 1\n",
    "\n",
    "for dx in dx_values:\n",
    "    errors = calculate_all_approximations(f, x, dx)\n",
    "    first_order_errors.append(errors[0])\n",
    "    second_order_errors.append(errors[1])\n",
    "    fourth_order_errors.append(errors[2])\n",
    "\n",
    "# Plot the errors for all three methods\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.loglog(dx_values, first_order_errors, label='First Order Error', marker='*', markersize=4)\n",
    "plt.loglog(dx_values, second_order_errors, label='Second Order Error', marker='x')\n",
    "plt.loglog(dx_values, fourth_order_errors, color='purple', label='Fourth Order Error', marker='.')\n",
    "\n",
    "annotation.slope_marker((1e-6, 5e-7), (1), ax=plt.gca())\n",
    "annotation.slope_marker((1e-4, 1e-9), (2), ax=plt.gca())\n",
    "annotation.slope_marker((1e-2, 1e-10), (4), ax=plt.gca())\n",
    "\n",
    "plt.xlabel(r'$\\Delta x$')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.title('Error in Different Approximations of the Derivative')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with finite difference approximations for derivatives, selecting the appropriate step size is important.For large values of $\\Delta x$ (tending to infinity) the error for all methods increase exponentially since the scale of the graph is a logarithmic one. This happens because the larger the step size, the less accurate the approximation becomes; important changes in the function's behavior over the interval are missed, and the truncation error grows as the higher-order terms in the Taylor series, which are ignored in the approximation, become substantial.\n",
    "\n",
    "For small values of $\\Delta x$ there is an area where the errors are at their minimum or low.After this point as $\\Delta x$ decreases further the error begins to increase again due to round-off errors. As $\\Delta x$ approaches $10^{-15}$ the error increases more significantly for higher order methods as the higher number of terms causes more loss of precision(floating point errors).This is especially evident in the plot for the fourth-order method.These behaviours are expected due to the reasons specified.\n",
    "\n",
    "The graph also reveals that the fourth-order central difference approximation not only achieves the lowest error in an optimal $\\Delta x$ range but also maintains this low error across the widest span of $\\Delta x$ values compared to lower-order methods. The disadvantages with this method however is as $\\Delta x$ becomes really small or large the error becomes higher than lower order methods.\n",
    "\n",
    "$\\Delta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 [40 marks]\n",
    "\n",
    "Consider the following function over the interval $[-1,1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_func(x):\n",
    "    # you can vary the 100 that appears below to change a key behaviour of this function\n",
    "    # feel free to update this function to pass it in as a variable\n",
    "    return 1.0 + (0.2 / (1.0 + (100.0 * x**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This function has been chosen because it is challenging to interpolate and to numerically integrate due to its rapid variation around $x=0$.\n",
    "\n",
    "But varying the number 100 that appears in this formula we can make it more/less of a challenging problem to interpolate/integrate.\n",
    "\n",
    "This is a two part question on (a) interpolation and (b) quadrature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use Sympy to differentiate this function and to provide indefinite and definite integrals to this function as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import Python's symbolic math library\n",
    "import sympy as sym\n",
    "# print maths nicely:\n",
    "sym.init_printing()\n",
    "# define the 'symbols' that will be our variables/parameters\n",
    "x = sym.symbols('x')\n",
    "# define a function \n",
    "expr = 1.0 + (0.2 / (1.0 + (100.0 * x**2))) \n",
    "# print it out to check it's correct\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is its indefinite integral\n",
    "sym.integrate(expr,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is its definite integral between bounds -1 and 1 - we store it in variable true_int\n",
    "# for later use in computing errors.\n",
    "true_int = sym.integrate(expr,(x,-1,1))\n",
    "print(true_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wanted to differentiate once, this is how you could do it\n",
    "sym.diff(expr, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the second derivative\n",
    "sym.diff(expr, x, x)\n",
    "# and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Interpolation\n",
    "\n",
    "- Firstly plot the function and explain how making the 100 that appears in the function larger and smaller (e.g. try halving and doubling it) changes the function's behaviour and why this behaviour makes this a challenging problem to interpolate. Please only consider piecewise polynomial interpolation with relatively low order polynomials within this question (you do not need to consider the \"Runge phenomena\" mentioned in class).\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- Now use piecewise polynomial interpolation to approximate the function, demonstrating how the error in your approximation varies with number of \"pieces\" (i.e. sub intervals) and polynomial order.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- Given that this function varies a lot in the middle of our interval, but less so away from the origin, investigate how you might be able to use non uniform sized sub-intervals in order to balance accuracy in your approximation vs the number of sub-intervals or function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the original function with a parameter to control the behavior\n",
    "def our_func(x, param=100.0):\n",
    "    return 1.0 + (0.2 / (1.0 + (param * x**2)))\n",
    "\n",
    "# Generate a range of x values\n",
    "x_values = np.linspace(-1, 1, 400)\n",
    "\n",
    "# Calculate y values for the original function and the variations\n",
    "y_original = our_func(x_values, 100)\n",
    "y_half = our_func(x_values, 50)\n",
    "y_double = our_func(x_values, 200)\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x_values, y_original, label='Parameter = 100', linewidth=2)\n",
    "plt.plot(x_values, y_half, label='Parameter = 50', linestyle='--')\n",
    "plt.plot(x_values, y_double, label='Parameter = 200', linestyle='-.')\n",
    "plt.title('Effect of Changing the Parameter in the Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly plot the function and explain how making the 100 that appears in the function larger and smaller (e.g. try halving and doubling it) changes the function's behaviour and why this behaviour makes this a challenging problem to interpolate. Please only consider piecewise polynomial interpolation with relatively low order polynomials within this question (you do not need to consider the \"Runge phenomena\" mentioned in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubling the parameter to 200 causes the function to change rapidly near $x = 0$ and this then becomes almost flat. Capturing this peak with a low-order polynomials is very difficult as subdivision would have be very precise around $x=0$ as the curve is so steep but less so elsewhere.Therefore if precise points are not chosen near $x=0$ the interpolation could overshoot or undershoot significantly\n",
    "\n",
    "Halving the parameter to 50 makes the function smoother and the peak is less sharp. However since the peak is more broader and the curve is still steep the curve is just as difficult to capture accurately and requires many sub intervals and higher order polynomials if possible so it is still difficult.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the target function as specified\n",
    "def our_func(x):\n",
    "    return 1.0 + (0.2 / (1.0 + (100.0 * x**2)))\n",
    "\n",
    "# Choose a range of x values\n",
    "x_values = np.linspace(-1, 1, 100)  # 100 points from -1 to 1\n",
    "y_values = our_func(x_values)\n",
    "\n",
    "# For visualization, plot the original function\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_values, y_values, label='Original Function')\n",
    "plt.title('Original Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('our_func(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subintervals = [4, 8, 16, 32]\n",
    "\n",
    "# Function to perform linear interpolation and plot only the interpolations\n",
    "def linear_interpolation_plot_only(subintervals):\n",
    "    # Prepare the figure for plotting the interpolations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the original function for reference\n",
    "    plt.plot(x_values, y_values, label='Original Function', linestyle='--')\n",
    "\n",
    "    # Interpolate and plot for the given number of subintervals\n",
    "    for pieces in subintervals:\n",
    "        # Define the x points for interpolation - include the endpoints\n",
    "        x_points = np.linspace(-1, 1, pieces + 1)\n",
    "        y_points = our_func(x_points)\n",
    "\n",
    "        # Create a linear interpolating function\n",
    "        linear_interp = interp1d(x_points, y_points, kind='linear')\n",
    "\n",
    "        # Interpolate the y values for the original x_values\n",
    "        y_interp = linear_interp(x_values)\n",
    "\n",
    "        # Plot the interpolated function\n",
    "        plt.plot(x_values, y_interp, label=f'Linear Interp. {pieces} pieces')\n",
    "\n",
    "    # Set titles and labels for the plot\n",
    "    plt.title('Linear Interpolation')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Perform and plot linear interpolation with 4, 8, 16 and 32 pieces\n",
    "linear_interpolation_plot_only(subintervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform quadratic interpolation and plot only the interpolations\n",
    "def quadratic_interpolation_plot_only(subintervals):\n",
    "    # Prepare the figure for plotting the interpolations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the original function for reference\n",
    "    plt.plot(x_values, y_values, label='Original Function', linestyle='--')\n",
    "\n",
    "    # Interpolate and plot for the given number of subintervals\n",
    "    for pieces in subintervals:\n",
    "        # Define the x points for interpolation - include the endpoints\n",
    "        x_points = np.linspace(-1, 1, pieces + 1)\n",
    "        y_points = our_func(x_points)\n",
    "\n",
    "        # Create a piecewise quadratic interpolating polynomial\n",
    "        quadratic_interp = interp1d(x_points, y_points, kind='quadratic')\n",
    "\n",
    "        # Interpolate the y values for the original x_values\n",
    "        y_interp = quadratic_interp(x_values)\n",
    "\n",
    "        # Plot the interpolated function\n",
    "        plt.plot(x_values, y_interp, label=f'Quadratic Interp. {pieces} pieces')\n",
    "\n",
    "    # Set titles and labels for the plot\n",
    "    plt.title('Quadratic Interpolation')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Perform and plot quadratic interpolation with 4, 8, 16 and 32 pieces\n",
    "quadratic_interpolation_plot_only(subintervals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to perform cubic spline interpolation and plot only the interpolations\n",
    "def cubic_spline_interpolation_plot_only(subintervals):\n",
    "    # Prepare the figure for plotting the interpolations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the original function for reference\n",
    "    plt.plot(x_values, y_values, label='Original Function', linestyle='--')\n",
    "\n",
    "    # Interpolate and plot for the given number of subintervals\n",
    "    for pieces in subintervals:\n",
    "        # Define the x points for interpolation - include the endpoints\n",
    "        x_points = np.linspace(-1, 1, pieces + 1)\n",
    "        y_points = our_func(x_points)\n",
    "\n",
    "        # Create a cubic spline interpolating polynomial\n",
    "        cubic_spline = CubicSpline(x_points, y_points)\n",
    "\n",
    "        # Interpolate the y values for the original x_values\n",
    "        y_interp = cubic_spline(x_values)\n",
    "\n",
    "        # Plot the interpolated function\n",
    "        plt.plot(x_values, y_interp, label=f'Cubic Spline {pieces} pieces')\n",
    "\n",
    "    # Set titles and labels for the plot\n",
    "    plt.title('Cubic Spline Interpolation')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Perform and plot cubic spline interpolation with 4, 8, 16 and 32 pieces\n",
    "cubic_spline_interpolation_plot_only(subintervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the L-infinity (max) norm of the error for linear interpolation\n",
    "def linear_interpolation_error_norm(subintervals):\n",
    "    error_norms = {}\n",
    "    for pieces in subintervals:\n",
    "        # Define the x points for interpolation - include the endpoints\n",
    "        x_points = np.linspace(-1, 1, pieces + 1)\n",
    "        y_points = our_func(x_points)\n",
    "\n",
    "        # Create a linear interpolating function\n",
    "        linear_interp = interp1d(x_points, y_points, kind='linear')\n",
    "\n",
    "        # Interpolate the y values for the original x_values\n",
    "        y_interp = linear_interp(x_values)\n",
    "\n",
    "        # Calculate the error\n",
    "        error = y_values - y_interp\n",
    "\n",
    "        # Compute the L-infinity norm of the error (maximum absolute error)\n",
    "        error_norms[pieces] = np.max(np.abs(error))\n",
    "    \n",
    "    return error_norms\n",
    "\n",
    "# Function to compute the L-infinity (max) norm of the error for quadratic interpolation\n",
    "def quadratic_interpolation_error_norm(subintervals):\n",
    "    error_norms = {}\n",
    "    for pieces in subintervals:\n",
    "        # Define the x points for interpolation - include the endpoints\n",
    "        x_points = np.linspace(-1, 1, pieces + 1)\n",
    "        y_points = our_func(x_points)\n",
    "\n",
    "        # Create a piecewise quadratic interpolating polynomial\n",
    "        quadratic_interp = interp1d(x_points, y_points, kind='quadratic')\n",
    "\n",
    "        # Interpolate the y values for the original x_values\n",
    "        y_interp = quadratic_interp(x_values)\n",
    "\n",
    "        # Calculate the error\n",
    "        error = y_values - y_interp\n",
    "\n",
    "        # Compute the L-infinity norm of the error (maximum absolute error)\n",
    "        error_norms[pieces] = np.max(np.abs(error))\n",
    "    \n",
    "    return error_norms\n",
    "\n",
    "# Calculate the L-infinity norm of the error for cubic spline interpolation\n",
    "def cubic_spline_error_norm(subintervals):\n",
    "    error_norms = {}\n",
    "    for pieces in subintervals:\n",
    "        # Define the x points for interpolation - include the endpoints\n",
    "        x_points = np.linspace(-1, 1, pieces + 1)\n",
    "        y_points = our_func(x_points)\n",
    "\n",
    "        # Create a cubic spline interpolating polynomial\n",
    "        cubic_spline = CubicSpline(x_points, y_points)\n",
    "\n",
    "        # Interpolate the y values for the original x_values\n",
    "        y_interp = cubic_spline(x_values)\n",
    "\n",
    "        # Calculate the error\n",
    "        error = y_values - y_interp\n",
    "\n",
    "        # Compute the L-infinity norm of the error (maximum absolute error)\n",
    "        error_norms[pieces] = np.max(np.abs(error))\n",
    "    \n",
    "    return error_norms\n",
    "\n",
    "\n",
    "# Calculate the L-infinity norm of the error for linear interpolation with 4, 8, 16 and 32 pieces\n",
    "linear_error_norms = linear_interpolation_error_norm(subintervals)\n",
    "# Now calculate the L-infinity norm of the error for quadratic interpolation\n",
    "quadratic_error_norms = quadratic_interpolation_error_norm(subintervals)\n",
    "cubic_error_norms = cubic_spline_error_norm(subintervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for plotting\n",
    "pieces_list = subintervals\n",
    "linear_error_list = [linear_error_norms[p] for p in pieces_list]\n",
    "quadratic_error_list = [quadratic_error_norms[p] for p in pieces_list]\n",
    "cubic_error_list = [cubic_error_norms[p] for p in pieces_list]\n",
    "\n",
    "# Plot the L-infinity norms of the error for all three interpolation methods\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(pieces_list, linear_error_list, marker='o', linestyle='-', color='r', label='Linear')\n",
    "plt.plot(pieces_list, quadratic_error_list, marker='s', linestyle='-', color='b', label='Quadratic')\n",
    "plt.plot(pieces_list, cubic_error_list, marker='^', linestyle='-', color='g', label='Cubic Spline')\n",
    "\n",
    "plt.title('L-infinity Norm of Error for Different Interpolation Methods')\n",
    "plt.xlabel('Number of pieces')\n",
    "plt.ylabel('L-infinity Norm of Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(pieces_list)  # Ensure x-ticks are the same for all\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area around $x=0$ is the steepest for the function so having more sub intervals in the center and less as $x$ moves away from $0$.Since the function is a bell curve, a non uniform but symmetrical distrbution is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create symmetric non-uniform sub-intervals focusing more points around the origin\n",
    "def create_symmetric_non_uniform_subintervals(num_points):\n",
    "    # Generate points from 0 to 1, apply a transformation to concentrate them near 0\n",
    "    positive_points = np.linspace(0, 1, num_points // 2 + 1) ** 3\n",
    "    # Mirror them to get -1 to 0 and concatenate, including 0 only once\n",
    "    non_uniform_points = np.concatenate((-positive_points[:0:-1], positive_points))\n",
    "    return non_uniform_points\n",
    "\n",
    "# Symmetric non-uniform sub-intervals with a focus around the origin\n",
    "num_subintervals = 32 \n",
    "symmetric_non_uniform_subintervals = create_symmetric_non_uniform_subintervals(num_subintervals)\n",
    "\n",
    "# Plot the symmetric non-uniform sub-intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(symmetric_non_uniform_subintervals, np.zeros_like(symmetric_non_uniform_subintervals), 'o', label=\"Symmetric Non-uniform Subintervals\")\n",
    "plt.title(\"Symmetric Non-Uniform Sub-Intervals\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to perform cubic spline interpolation with symmetric non-uniform subintervals and plot\n",
    "def cubic_spline_symmetric_non_uniform_plot(subinterval_counts):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the original function\n",
    "    plt.plot(x_values, y_values, label=\"Original Function\", linestyle='--', marker='x')\n",
    "\n",
    "    # Perform interpolation for each specified subinterval count\n",
    "    for count in subinterval_counts:\n",
    "        # Create symmetric non-uniform sub-intervals for the current count\n",
    "        symmetric_subintervals = create_symmetric_non_uniform_subintervals(count)\n",
    "        # Evaluate the function at these sub-intervals\n",
    "        symmetric_y_points = our_func(symmetric_subintervals)\n",
    "        # Create a cubic spline interpolator\n",
    "        symmetric_cubic_spline = CubicSpline(symmetric_subintervals, symmetric_y_points)\n",
    "        # Interpolate the values using the spline\n",
    "        symmetric_y_interp = symmetric_cubic_spline(x_values)\n",
    "        # Plot the interpolation\n",
    "        plt.plot(x_values, symmetric_y_interp, label=f\"Cubic Spline {count} points\")\n",
    "\n",
    "    # Set up plot labels and legend\n",
    "    plt.title(\"Cubic Spline Interpolation with Symmetric Non-uniform Sub-Intervals\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Perform interpolation and plot for 4, 8, 16, 32 subintervals\n",
    "cubic_spline_symmetric_non_uniform_plot([4, 8, 16, 32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cubic_spline_symmetric_non_uniform_plot([32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense grid for error calculation\n",
    "x_dense = np.linspace(-1, 1, 1000)\n",
    "y_true = our_func(x_dense)\n",
    "\n",
    "# Dictionary to store L_max norms for each subinterval count\n",
    "l_max_norms = {}\n",
    "\n",
    "# Compute the L_max norm for each subinterval count\n",
    "for count in subintervals:\n",
    "    # Create non-uniform sub-intervals\n",
    "    sub_interval_points = create_symmetric_non_uniform_subintervals(count)\n",
    "    # Evaluate the function at these sub-intervals\n",
    "    y_sub_interval = our_func(sub_interval_points)\n",
    "    # Create a cubic spline interpolator\n",
    "    cubic_spline = CubicSpline(sub_interval_points, y_sub_interval)\n",
    "    # Interpolate the values using the spline on the dense grid\n",
    "    y_interp = cubic_spline(x_dense)\n",
    "    # Compute the L_max norm\n",
    "    l_max_norm = np.max(np.abs(y_true - y_interp))\n",
    "    l_max_norms[count] = l_max_norm\n",
    "\n",
    "# Plot the L_max norms\n",
    "subinterval_counts = list(l_max_norms.keys())\n",
    "l_max_values = list(l_max_norms.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(subinterval_counts, l_max_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('L_max Norms for Different Sub-Interval Counts')\n",
    "plt.xlabel('Number of Sub-Intervals')\n",
    "plt.ylabel('L_max Norm')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a non-uniform distribution it is clear that using a non-uniformly distributed number of sub intervals can be very beneficial if the sub interval count is high enough. Looking at the cubic spline interpolation it is clear that the error is very close to zero. This is a much more significant improvement than for the same interpolation for a uniformly distributed sub-intervals function. This is because Non-uniform sub-intervals allow the interpolation to adapt locally. Where the function changes rapidly, closer nodes mean that the cubic polynomial doesn't have to stretch as far to approximate the function, leading to higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Quadrature\n",
    "\n",
    "- Use some of the methods we considered in lectures to approximate the definite integral to this function over the interval stated above. Compare their performance in terms of error vs number of sub-intervals.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- How does the convergence analysis change if you consider error vs number of function evaluations? By considering the implementations of the Trapezoidal and Simpsons rules from lectures, how might you be able to update our basic implementations in order to minimise function evaluations (e.g. through re-use in one sub interval of function evaluations from the previous sub-interval) - try implementing and verifying these ideas.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- Investigate the use of non-uniform sub-interval size in order to further optimise the error vs number of function evaluations relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
